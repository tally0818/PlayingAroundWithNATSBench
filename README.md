# üß† Exploring Neural Architecture Search with NATS-Bench

This Jupyter Notebook demonstrates a hands-on exploration of **Neural Architecture Search (NAS)** using the **NATS-Bench** benchmark.

It walks through three major areas:
1. **Environment Setup & Dataset Preparation**
2. **Multi-Trial NAS Algorithms**
3. **Zero-Cost Proxies**

---

## üì¶ 0. Environment Setup & Dataset Preparation

The notebook begins by:
- Installing dependencies and setting up the environment.
- Downloading and initializing the NATS-Bench dataset.

---

## üîÅ 1. Multi-Trial NAS Algorithms

This section covers classic multi-trial NAS methods:
- **Random Search**
- **Regularized Evolution (EA)**
- **Reinforcement Learning-based Search**

Key analyses include:
- Accuracy & loss visualization across different strategies.
- Comparison of algorithm performance on NATS-Bench.
- Insights into why regularized evolution tends to outperform others.

---

## ‚ö° 2. Zero-Cost NAS Proxies

This part introduces:
- Fast performance prediction techniques without full training.
- Initial experiments and interpretations of zero-cost proxy effectiveness.

---

## üìä Visualization & Analysis

Throughout the notebook, insightful plots and metrics are provided to:
- Understand algorithm behavior.
- Analyze operation preferences and search dynamics.
- Evaluate NAS performance effectively.

---

## üìñ References

[NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size](https://arxiv.org/abs/2009.00437)

[NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING](https://arxiv.org/pdf/1611.01578)

[Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/pdf/1707.07012)

[Neural Architecture Search: Insights from 1000 Papers](https://arxiv.org/pdf/2301.08727)

[Regularized Evolution for Image Classifier Architecture Search](https://arxiv.org/pdf/1802.01548)

[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)

[Zero-Cost Proxies for Lightweight NAS](https://arxiv.org/abs/2101.08134)

[Pruning neural networks without any data by iteratively conserving synaptic flow](https://arxiv.org/abs/2006.05467)

[SNIP: Single-shot Network Pruning based on Connection Sensitivity](https://arxiv.org/abs/1810.02340)

